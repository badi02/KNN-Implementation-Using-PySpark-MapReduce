{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "9KiqMzkIqioE",
        "outputId": "45d7e672-72da-4ed4-980a-0eb6ce526c95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "\r0% [Waiting for headers] [1 InRelease 14.2 kB/110 kB 13%] [Connected to cloud.r-project.org (52.85.1\r                                                                                                    \rHit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [1 InRelease 110 kB/110 kB 100%] [Connected to cloud.r-project.org (52.85.151.129)] [Connecting t\r                                                                                                    \rGet:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "\r                                                                                                    \rHit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "\r                                                                                                    \rGet:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease [18.1 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,599 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,305 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,326 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,046 kB]\n",
            "Get:15 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main Sources [2,257 kB]\n",
            "Get:16 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main amd64 Packages [1,158 kB]\n",
            "Fetched 8,942 kB in 3s (3,487 kB/s)\n",
            "Reading package lists... Done\n",
            "sample_data  spark-3.1.1-bin-hadoop3.2\tspark-3.1.1-bin-hadoop3.2.tgz\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x79f554d34b20>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://fbc79bc1f90e:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!apt-get update # Update apt-get repository.\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # Install Java.\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz # Download Apache Sparks.\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz # Unzip the tgz file.\n",
        "!pip install -q findspark # Install findspark. Adds PySpark to the System path during runtime.\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "!ls\n",
        "\n",
        "# Initialize findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Create a PySpark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "2zebLuQ_siGH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "fl_CaVZ-EONg"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "import math\n",
        "\n",
        "# Initialize Spark context\n",
        "sc = SparkContext(\"local\", \"KNN\")\n",
        "\n",
        "# Example data: Replace this with your actual data\n",
        "data = [(2, [2.6, 8]), (1, [18.655, 11.20]), (3, [14, 5])]\n",
        "query_point = [10.6, 1.5]\n",
        "k = 2\n",
        "\n",
        "# Create an RDD from the data\n",
        "data_rdd = sc.parallelize(data)\n",
        "\n",
        "# Calculate Euclidean distance function\n",
        "def calculate_distance(point1, point2):\n",
        "    if len(point1) != len(point2):\n",
        "        raise ValueError(\"Points must have the same number of dimensions\")\n",
        "\n",
        "    squared_distances = [(a - b) ** 2 for a, b in zip(point1, point2)]\n",
        "    euclidean_distance = math.sqrt(sum(squared_distances))\n",
        "    return euclidean_distance\n",
        "\n",
        "# Map function to calculate distances\n",
        "def map_function(data_point):\n",
        "    data_point_id, coordinates = data_point\n",
        "    distance = calculate_distance(coordinates, query_point)\n",
        "    print('==========================================================\\n')\n",
        "    print('RESULT OF MAP FUNCTION:')\n",
        "    print(f'KEY: {data_point_id}\\n VALUE: {distance}')\n",
        "    return (data_point_id, distance)\n",
        "\n",
        "# Apply map function\n",
        "distances_rdd = data_rdd.map(map_function)\n",
        "collected_distances = distances_rdd.collect()\n",
        "print('==========================================================\\n')\n",
        "print('RESULT OF MAP FUNCTION:\\n')\n",
        "print(collected_distances)\n",
        "# Reduce function to find top k distances\n",
        "def reduce_function(x, y):\n",
        "    # Combine distances and keep the smallest k\n",
        "    combined_distances = sorted(x + y, key=lambda x: x[1])[:k]\n",
        "    print('==========================================================\\n')\n",
        "    print('RESULT OF REDUCE FUNCTION:')\n",
        "    print(f'KEY: {data_point_id}\\n VALUE: {combined_distances}')\n",
        "    return combined_distances\n",
        "\n",
        "# Aggregate distances using reduce function\n",
        "top_k_distances = distances_rdd.reduceByKey(reduce_function)\n",
        "\n",
        "# Collect the results\n",
        "results = top_k_distances.collect()\n",
        "\n",
        "# Print the results\n",
        "for result in results:\n",
        "    data_point_id, top_k = result\n",
        "    print(f\"Data Point ID: {data_point_id}, Top K Distances: {top_k}\")\n",
        "\n",
        "# Stop Spark context\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWgk_4MoER-0",
        "outputId": "9b610b56-30e9-480d-ccd1-bd67dc2b92b0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================\n",
            "\n",
            "RESULT OF MAP FUNCTION:\n",
            "\n",
            "[(2, 10.307764064044152), (1, 12.608450539221701), (3, 4.879549159502341)]\n",
            "Data Point ID: 2, Top K Distances: 10.307764064044152\n",
            "Data Point ID: 1, Top K Distances: 12.608450539221701\n",
            "Data Point ID: 3, Top K Distances: 4.879549159502341\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "HuhLdd_mM14A"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XIX8JydZPjCy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}